{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317eea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Projection onto the future ###\n",
    "def proj_omega(y,x_1,x_2):\n",
    "    ## Gram-Schmidt to find projection\n",
    "    e_1 = x_1/np.linalg.norm(x_1)\n",
    "    e_2 = x_2 - (x_2.T@e_1)*e_1\n",
    "    e_2 = e_2/np.linalg.norm(e_2)\n",
    "    \n",
    "    y_proj_1 = (y.T@e_1)*e_1\n",
    "    y_proj_2 = (y.T@e_2)*e_2\n",
    "    \n",
    "    return y_proj_1 + y_proj_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1997e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_vals(A,b,x_it,itr = 250,gamma = 1):\n",
    "    \n",
    "    error = np.zeros(itr)\n",
    "    m = np.shape(A)[0]\n",
    "    \n",
    "    proj = []\n",
    "    \n",
    "    for j in range(m): #Inner iteration for Kaczmarz updates\n",
    "        a = A[j,:]\n",
    "        proj_val = np.abs(b[j] - a@x_it)/np.linalg.norm(a)\n",
    "        proj = proj + [proj_val]\n",
    "    \n",
    "    return proj\n",
    "\n",
    "\n",
    "## Finding indices of high values\n",
    "def high_proj(a, number = 5):\n",
    "    res = sorted(range(len(a)), key = lambda sub: a[sub])[-number:]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5632bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TIHT_HOSVD_proj_high(AA,yy,X,r,lamda = 1, itr = 100, num = 4): \n",
    "    \n",
    "    n = np.shape(X)\n",
    "    X_ravel = np.ravel(X)\n",
    "    \n",
    "    error = np.zeros(itr)\n",
    "    \n",
    "    vXX = torch.randn(n)*0\n",
    "\n",
    "    for j in range(itr):\n",
    "        \n",
    "        proj = proj_vals(AA,yy,np.array(vectorize_tl(vXX)))\n",
    "                           \n",
    "        remove_row = high_proj(proj, number = num)\n",
    "        A_1 = np.delete(AA,remove_row,axis = 0)\n",
    "        y_1 = np.delete(yy,remove_row,axis = 0)\n",
    "    \n",
    "        WW = np.array(vectorize_tl(vXX)) + (m/(m-num))*lamda* np.matmul(A_1.T, (y_1 - np.matmul(A_1, np.array(vectorize_tl(vXX)))))\n",
    "        WW = torch.reshape(torch.tensor(WW), n)\n",
    "        vXX = HOSVD_rank_app(WW,r)\n",
    "        error[j] = np.linalg.norm(vectorize_tl(vXX)- X_ravel)/np.linalg.norm(X_ravel)\n",
    "          \n",
    "    return vXX, error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b624824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_TIHT_comp(AA,yy,X,r,lamda,mu = 1,itr=250, num_high = 5):\n",
    "    \n",
    "    n = np.shape(X)\n",
    "    X_ravel = np.ravel(X)\n",
    "    m = np.shape(AA)[0]\n",
    "    \n",
    "    error = np.zeros(itr)\n",
    "    error_unclip = np.zeros(itr)\n",
    "    delta_1_l = np.zeros(itr)\n",
    "    rho_1_l = np.zeros(itr)\n",
    "    rate_1_l = np.zeros(itr)\n",
    "    delta_2_l = np.zeros(itr)\n",
    "    rho_2_l = np.zeros(itr)\n",
    "    rate_2_l = np.zeros(itr)\n",
    "    rate_1_init = 1/np.linalg.norm(X_ravel)\n",
    "    rate_2_init = 1/np.linalg.norm(X_ravel)\n",
    "    \n",
    "    vXX = torch.randn(n)*0\n",
    "    vXX_unclip = torch.randn(n)*0\n",
    "\n",
    "    for j in range(itr):\n",
    "        \n",
    "        delta_1 = lamda*(np.linalg.norm(np.matmul(AA, vectorize_tl(vXX_unclip) - X_ravel))/np.linalg.norm(vectorize_tl(vXX_unclip) - X_ravel))**2\n",
    "        WW_unclip = np.array(vectorize_tl(vXX_unclip)) + mu*lamda* np.matmul(AA.T, (yy - np.matmul(AA, np.array(vectorize_tl(vXX_unclip)))))\n",
    "        WW_unclip = torch.reshape(torch.tensor(WW_unclip), n)\n",
    "        R_t = vectorize_tl(vXX_unclip) - X_ravel\n",
    "        vXX_unclip = HOSVD_rank_app(WW_unclip,r)\n",
    "        R_t1 = vectorize_tl(vXX_unclip) - X_ravel\n",
    "        rho_1 = (np.linalg.norm(proj_omega((1/np.sqrt(m)*AA.T)@(1/np.sqrt(m)*AA)@R_t,R_t,R_t1))/np.linalg.norm((1/np.sqrt(m)*AA)@R_t))**2\n",
    "        rate_1 = 2*np.sqrt(1-(2-mu*rho_1)*mu*delta_1)\n",
    "        delta_1_l[j] = delta_1\n",
    "        rho_1_l[j] = rho_1\n",
    "        rate_1_l[j] = rate_1*rate_1_init\n",
    "        rate_1_init = rate_1_l[j]\n",
    "        error_unclip[j] = np.linalg.norm(vectorize_tl(vXX_unclip)- X_ravel)/np.linalg.norm(X_ravel)\n",
    "\n",
    "        #With Clipping\n",
    "        proj = proj_vals(AA,yy,np.array(vectorize_tl(vXX)))\n",
    "        delta = AA@vectorize_tl(vXX) - yy\n",
    "\n",
    "\n",
    "        remove_row = high_proj(proj, number = num_high)\n",
    "        A_1 = np.delete(AA,remove_row,axis = 0)\n",
    "        y_1 = np.delete(yy,remove_row,axis = 0)\n",
    "        num = num_high\n",
    "        \n",
    "        delta_2 = (m/(m-num))*lamda*(np.linalg.norm(np.matmul(A_1, vectorize_tl(vXX) - X_ravel))/np.linalg.norm(vectorize_tl(vXX) - X_ravel))**2\n",
    "        WW = np.array(vectorize_tl(vXX)) + (m/(m-num))*lamda*mu*np.matmul(A_1.T, (y_1 - np.matmul(A_1, np.array(vectorize_tl(vXX)))))\n",
    "        WW = torch.reshape(torch.tensor(WW), n)\n",
    "        R_t = vectorize_tl(vXX) - X_ravel\n",
    "        vXX = HOSVD_rank_app(WW,r)\n",
    "        R_t1 = vectorize_tl(vXX) - X_ravel\n",
    "        rho_2 = (m/(m-num))*lamda*(np.linalg.norm(proj_omega(A_1.T@A_1@R_t,R_t,R_t1))/np.linalg.norm(A_1@R_t))**2\n",
    "        rate_2 =  2*np.sqrt(1-(2-mu*rho_2)*mu*delta_2)\n",
    "        delta_2_l[j] = delta_2\n",
    "        rho_2_l[j] = rho_2\n",
    "        rate_2_l[j] = rate_2*rate_2_init\n",
    "        rate_2_init = rate_2_l[j]\n",
    "        error[j] = np.linalg.norm(vectorize_tl(vXX)- X_ravel)/np.linalg.norm(X_ravel)\n",
    "        \n",
    "    return delta_1_l, delta_2_l, rho_1_l, rho_2_l, rate_1_l, rate_2_l, error_unclip, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f371a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_TIHT(AA,yy,X,r,lamda,mu = 1,itr=250, numb_high=5):\n",
    "    \n",
    "    n = np.shape(X)\n",
    "    X_ravel = np.ravel(X)\n",
    "    \n",
    "    error = np.zeros(itr)\n",
    "    vXX = torch.randn(n)*0\n",
    "    m = np.shape(AA)[0]\n",
    "    \n",
    "    for j in range(itr):\n",
    "        proj = proj_vals(AA,yy,np.array(vectorize_tl(vXX)))\n",
    "        delta = AA@vectorize_tl(vXX) - yy\n",
    "        \n",
    "        remove_row = high_proj(proj, number = numb_high)\n",
    "        A_1 = np.delete(AA,remove_row,axis = 0)\n",
    "        y_1 = np.delete(yy,remove_row,axis = 0)\n",
    "        num = numb_high\n",
    "        \n",
    "        WW = np.array(vectorize_tl(vXX)) + (m/(m-num))*lamda*mu*np.matmul(A_1.T, (y_1 - np.matmul(A_1, np.array(vectorize_tl(vXX)))))\n",
    "        WW = torch.reshape(torch.tensor(WW), n)\n",
    "        vXX = HOSVD_rank_app(WW,r)\n",
    "        error[j] = np.linalg.norm(vectorize_tl(vXX)- X_ravel)/np.linalg.norm(X_ravel)\n",
    "    \n",
    "    return vXX, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2010888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_KZTIHT(AA,yy,X,r,gamma = 1,lamda = 1,itr=250, numb_high=5):\n",
    "    \n",
    "    n = np.shape(X)\n",
    "    X_ravel = np.ravel(X)\n",
    "    n_dim =  np.shape(AA)[1]\n",
    "    \n",
    "    AA,yy = row_normalised_mx(AA,yy,n_dim)\n",
    "    \n",
    "    error = np.zeros(itr)\n",
    "    y = torch.randn(n)*0\n",
    "    m = np.shape(AA)[0]\n",
    "    converge = True\n",
    "    k = 0\n",
    "    \n",
    "    while converge == True and k < itr:\n",
    "        proj = proj_vals(AA,yy,np.array(vectorize_tl(y)))\n",
    "        \n",
    "        remove_row = high_proj(proj, number = numb_high)\n",
    "        A_1 = np.delete(AA,remove_row,axis = 0)\n",
    "        y_1 = np.delete(yy,remove_row,axis = 0)\n",
    "        num = numb_high\n",
    "        \n",
    "        gamma = gamma*n_dim/(m-num)\n",
    "        y_old = y\n",
    "        \n",
    "        try:\n",
    "            t = permutation(np.arange(m-num))\n",
    "            for j in range(m-num): #Inner iteration for Kaczmarz updates\n",
    "                a = A_1[t[j],:]\n",
    "                y = y + gamma*(y_1[t[j]] - a@y)*a/(np.linalg.norm(a)**2)    \n",
    "            y = y_old + lamda*(y - y_old)   \n",
    "            WW = torch.reshape(torch.tensor(y), n)\n",
    "            y = vectorize_tl(HOSVD_rank_app(WW,r))\n",
    "            error[k] = np.linalg.norm(vectorize_np(y)-x)/np.linalg.norm(x)\n",
    "            k = k+1 \n",
    "                \n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Doesn't converge\")\n",
    "            y = np.zeros(np.shape(x)[0])-1\n",
    "            error = np.zeros(itr)+np.inf\n",
    "            converge = False\n",
    "            \n",
    "    \n",
    "    return y, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "871dcd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_TIHT_CP(AA,yy,X,r,lamda,mu = 1,itr=250, numb_high=5):\n",
    "    \n",
    "    n = np.shape(X)\n",
    "    X_ravel = np.ravel(X)\n",
    "    \n",
    "    error = np.zeros(itr)\n",
    "    vXX = torch.randn(n)*0\n",
    "    vXX= np.array((vectorize_tl(vXX)))\n",
    "    m = np.shape(AA)[0]\n",
    "    \n",
    "    for j in range(itr):\n",
    "        proj = proj_vals(AA,yy,np.ravel(vXX))\n",
    "        delta = AA@np.ravel(vXX) - yy\n",
    "        \n",
    "        remove_row = high_proj(proj, number = numb_high)\n",
    "        A_1 = np.delete(AA,remove_row,axis = 0)\n",
    "        y_1 = np.delete(yy,remove_row,axis = 0)\n",
    "        num = numb_high\n",
    "        \n",
    "        WW = np.array(np.ravel(vXX)) + (m/(m-num))*lamda*mu*np.matmul(A_1.T, (y_1 - np.matmul(A_1, np.array(np.ravel(vXX)))))\n",
    "        WW = torch.reshape(torch.tensor(WW), n)\n",
    "        vXX = CP_rank_app(WW,r)\n",
    "        error[j] = np.linalg.norm(vectorize_np(vXX)-X_ravel)/np.linalg.norm(X_ravel)\n",
    "    \n",
    "    return vXX, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "614429ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_KZTIHT_CP(A,b,X,r,gamma = 1,lamda = 1,itr=250, numb_high=5):\n",
    "    \n",
    "    error = np.zeros(itr)\n",
    "    m = np.shape(A)[0]\n",
    "    n_dim =  np.shape(A)[1]\n",
    "    \n",
    "    n = np.shape(X)\n",
    "    x = np.ravel(X)\n",
    "    A,b = row_normalised_mx(A,b,n_dim)\n",
    "    \n",
    "    y = np.zeros(np.shape(x)[0])    \n",
    "    converge = True\n",
    "    k = 0\n",
    "    m_1 = m - numb_high\n",
    "\n",
    "    \n",
    "    gamma = gamma*n_dim/m_1\n",
    "            \n",
    "    while converge == True and k < itr:\n",
    "        proj = proj_vals(A,b,np.ravel(y))\n",
    "        remove_row = high_proj(proj, number = numb_high)\n",
    "        \n",
    "        A_1 = np.delete(A,remove_row,axis = 0)\n",
    "        b_1 = np.delete(b,remove_row,axis = 0)\n",
    "        \n",
    "        try:\n",
    "            y_old = y\n",
    "            t = permutation(np.arange(m_1))\n",
    "            for j in range(m_1): #Inner iteration for Kaczmarz updates\n",
    "                a = A_1[t[j],:]\n",
    "                y = y + gamma*(b_1[t[j]] - a@y)*a/(np.linalg.norm(a)**2)     \n",
    "                \n",
    "            y = y_old + lamda*(y - y_old)\n",
    "            WW = torch.reshape(torch.tensor(y), n)\n",
    "            y = CP_rank_app(WW,r)\n",
    "            error[k] = np.linalg.norm(vectorize_np(y)-x)/np.linalg.norm(x)\n",
    "            y = vectorize_np(y)\n",
    "            k = k+1 \n",
    "                             \n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Doesn't converge\")\n",
    "            y = np.zeros(np.shape(x)[0])-1\n",
    "            error = np.zeros(itr)+np.inf\n",
    "            converge = False\n",
    "            \n",
    "    \n",
    "    return y, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "845b361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_TIHT_comp(AA,yy,X,r,lamda,mu = 1,itr=250, num_high = 5):\n",
    "    \n",
    "    n = np.shape(X)\n",
    "    X_ravel = np.ravel(X)\n",
    "    m = np.shape(AA)[0]\n",
    "    \n",
    "    error = np.zeros(itr)\n",
    "    error_unclip = np.zeros(itr)\n",
    "    delta_1_l = np.zeros(itr)\n",
    "    rho_1_l = np.zeros(itr)\n",
    "    rate_1_l = np.zeros(itr)\n",
    "    delta_2_l = np.zeros(itr)\n",
    "    rho_2_l = np.zeros(itr)\n",
    "    rate_2_l = np.zeros(itr)\n",
    "    rate_1_init = 1/np.linalg.norm(X_ravel)\n",
    "    rate_2_init = 1/np.linalg.norm(X_ravel)\n",
    "    \n",
    "    vXX = torch.randn(n)*0\n",
    "    vXX_unclip = torch.randn(n)*0\n",
    "\n",
    "    for j in range(itr):\n",
    "        \n",
    "        delta_1 = lamda*(np.linalg.norm(np.matmul(AA, vectorize_tl(vXX_unclip) - X_ravel))/np.linalg.norm(vectorize_tl(vXX_unclip) - X_ravel))**2\n",
    "        WW_unclip = np.array(vectorize_tl(vXX_unclip)) + mu*lamda* np.matmul(AA.T, (yy - np.matmul(AA, np.array(vectorize_tl(vXX_unclip)))))\n",
    "        WW_unclip = torch.reshape(torch.tensor(WW_unclip), n)\n",
    "        R_t = vectorize_tl(vXX_unclip) - X_ravel\n",
    "        vXX_unclip = HOSVD_rank_app(WW_unclip,r)\n",
    "        R_t1 = vectorize_tl(vXX_unclip) - X_ravel\n",
    "        rho_1 = (np.linalg.norm(proj_omega((1/np.sqrt(m)*AA.T)@(1/np.sqrt(m)*AA)@R_t,R_t,R_t1))/np.linalg.norm((1/np.sqrt(m)*AA)@R_t))**2\n",
    "        rate_1 = 2*np.sqrt(1-(2-mu*rho_1)*mu*delta_1)\n",
    "        delta_1_l[j] = delta_1\n",
    "        rho_1_l[j] = rho_1\n",
    "        rate_1_l[j] = rate_1*rate_1_init\n",
    "        rate_1_init = rate_1_l[j]\n",
    "        error_unclip[j] = np.linalg.norm(vectorize_tl(vXX_unclip)- X_ravel)/np.linalg.norm(X_ravel)\n",
    "\n",
    "        #With Clipping\n",
    "        proj = proj_vals(AA,yy,np.array(vectorize_tl(vXX)))\n",
    "        delta = AA@vectorize_tl(vXX) - yy\n",
    "\n",
    "\n",
    "        remove_row = high_proj(proj, number = num_high)\n",
    "        A_1 = np.delete(AA,remove_row,axis = 0)\n",
    "        y_1 = np.delete(yy,remove_row,axis = 0)\n",
    "        num = num_high\n",
    "        \n",
    "        delta_2 = (m/(m-num))*lamda*(np.linalg.norm(np.matmul(A_1, vectorize_tl(vXX) - X_ravel))/np.linalg.norm(vectorize_tl(vXX) - X_ravel))**2\n",
    "        WW = np.array(vectorize_tl(vXX)) + (m/(m-num))*lamda*mu*np.matmul(A_1.T, (y_1 - np.matmul(A_1, np.array(vectorize_tl(vXX)))))\n",
    "        WW = torch.reshape(torch.tensor(WW), n)\n",
    "        R_t = vectorize_tl(vXX) - X_ravel\n",
    "        vXX = HOSVD_rank_app(WW,r)\n",
    "        R_t1 = vectorize_tl(vXX) - X_ravel\n",
    "        rho_2 = (m/(m-num))*lamda*(np.linalg.norm(proj_omega(A_1.T@A_1@R_t,R_t,R_t1))/np.linalg.norm(A_1@R_t))**2\n",
    "        rate_2 =  2*np.sqrt(1-(2-mu*rho_2)*mu*delta_2)\n",
    "        delta_2_l[j] = delta_2\n",
    "        rho_2_l[j] = rho_2\n",
    "        rate_2_l[j] = rate_2*rate_2_init\n",
    "        rate_2_init = rate_2_l[j]\n",
    "        error[j] = np.linalg.norm(vectorize_tl(vXX)- X_ravel)/np.linalg.norm(X_ravel)\n",
    "        \n",
    "    return delta_1_l, delta_2_l, rho_1_l, rho_2_l, rate_1_l, rate_2_l, error_unclip, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d4a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
